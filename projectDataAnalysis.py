
Automatically generated by Colaboratory.


from google.colab import files
uploaded = files.upload()

import io
import pandas as pd

df_train = pd.read_csv(io.StringIO(uploaded["train.csv"].decode('utf-8')))
print(df_train.head())

# Missing values count in each columns
missing_count = (df_train.isnull().sum() / len(df_train)) * 100
# Filter to find the missing count > 0
# nummbers are the percentage that is missing
missing_count = missing_count[missing_count > 0]
print(missing_count.sort_values())

# Handling the missing columns
import seaborn as sns
sns.distplot(df_train['SalePrice'])

# Most machine learning algorith works well with data which are normally distributed
# Transfroming the target (SalePrice) variable by taking log scale
import numpy as np

target = np.log(df_train['SalePrice'])
sns.distplot(target)

# Finding numerical features
numeric_data = df_train.select_dtypes(include = [np.number])


# Finding categorical features
categorical_data = df_train.select_dtypes(exclude = [np.number])

# printing how many numerical and categorical features do we have
print('There are {0} numerical and {1} categorical features in the training data'.\
      format(numeric_data.shape[1], categorical_data.shape[1]))

# Correlation plot of the features in numeric_data

corr = numeric_data.corr()

# Plot the correlation matrix

sns.heatmap(corr)
# Lighter colours are more correlated
# both triangles are the same

# Correlation plot for the numerical features
# Remove the Id column from numeric_data

del numeric_data['Id']
numeric_data.columns

# Commented out IPython magic to ensure Python compatibility.
# When was the property sold in which year and month

import matplotlib.pyplot as plt
# %matplotlib inline

df_train.groupby(['YrSold', 'MoSold']).Id.count().plot(kind = 'bar', figsize = (14,4))
plt.title('When was the property sold?')
plt.show()

# Where are the property located?

df_train.groupby('Neighborhood').Id.count().\
    sort_values().\
    plot(kind = 'bar', figsize = (10, 4))
plt.title('Where are the most of the property located?')
plt.show()

# Distribution of numerical features

f = pd.melt(df_train, value_vars = sorted(numeric_data))
g = sns.FacetGrid(f, col = 'variable', col_wrap = 4, sharex = False, sharey = False)
g = g.map(sns.distplot, 'value')

# Conversion from numeric feature to category features

df_train['MSSubClass'] = df_train.MSSubClass.apply(lambda x: str(x))
df_train['MoSold'] = df_train.MoSold.apply(lambda x: str(x))
df_train['YrSold'] = df_train.YrSold.apply(lambda x: str(x))

# Finding numerical features
numeric_data = df_train.select_dtypes(include = [np.number])


# Finding categorical features
categorical_data = df_train.select_dtypes(exclude = [np.number])

# printing how many numerical and categorical features do we have
print('There are {0} numerical and {1} categorical features in the training data'.\
      format(numeric_data.shape[1], categorical_data.shape[1]))

# Let's plot count of categorical features
# y-axis how many times the value has been in the data set
# x-axis different values for base condition

f = pd.melt(df_train, value_vars = sorted(categorical_data))
g = sns.FacetGrid(f, col = 'variable', col_wrap = 4, sharex = False, sharey=False)
plt.xticks(rotation = 'vertical')
g = g.map(sns.countplot, 'value')
[plt.setp(ax.get_xticklabels(), rotation = 60) for ax in g.axes.flat]
g.fig.tight_layout()
plt.show()

df_train.Alley.replace({'Grvl':1, 'Pave':2}, inplace = True)

df_train['Alley'].head()
#The reason is because the 90% is nan values that why so we are gonna replace it with unique  method so we have a result

df_train['Alley'].unique()

# Lot Shape
df_train.LotShape.replace({'Reg':1, 'IR1:':2, 'IR2':3, 'IR3': 4}, inplace = True)

# Land Contour
df_train.LandContour.replace({'Low':1, 'HLS:':2, 'Bnk':3, 'Lv1': 4}, inplace = True)

# Utilities
df_train.Utilities.replace({'ELO':1, 'NoSeWa:':2, 'NoSewr':3, 'AllPub': 4}, inplace = True)

# Land Slope
df_train.LandSlope.replace({'Sev':1, 'Mod:':2, 'Gt1':3,}, inplace = True)

# Exterior Quality
df_train.ExterQual.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Exterior Condition
df_train.ExterCond.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Basement Quality
df_train.BsmtQual.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Basement Condition
df_train.BsmtCond.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Basement Exposure
df_train.BsmtExposure.replace({'No':1, 'Mn:':2, 'Av':3, 'Gd': 4}, inplace = True)

# Finished Basement 1 Rating
df_train.BsmtFinType1.replace({'Unf':1, 'LwQ:':2, 'Rec':3, 'BLQ': 4, 'ALQ':5, 'GLQ':6}, inplace = True)

# Finished Basement 2 Rating
df_train.BsmtFinType2.replace({'Unf':1, 'LwQ:':2, 'Rec':3, 'BLQ': 4, 'ALQ':5, 'GLQ':6}, inplace = True)

# Heating Quality and Condition
df_train.HeatingQC.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Kitchen Quality
df_train.KitchenQual.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Home functionality
df_train.Functional.replace({'Sal':1, 'Sev:':2, 'Maj2':3, 'Maj1': 4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}, inplace = True)

# Fireplace Quality
df_train.FireplaceQu.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Garage Finish
df_train.GarageFinish.replace({'Unf':1, 'RFn:':2, 'Fin':3}, inplace = True)

# Garage Quality
df_train.GarageQual.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Garage Condition
df_train.GarageCond.replace({'Po':1, 'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

# Paved Driveway
df_train.PavedDrive.replace({'N':1, 'P:':2, 'Y':3}, inplace = True)

# Pool Quality
df_train.PoolQC.replace({'Fa:':2, 'TA':3, 'Gd': 4, 'Ex':5}, inplace = True)

cat_to_num_features = ['Alley', 'LotShape', 'LandContour', 'Utilities', 'LandSlope', 'ExterQual',
                       'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 
                       'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC']

df_train[cat_to_num_features] = df_train[cat_to_num_features].fillna(0)

# Finding numerical features
numeric_data = df_train.select_dtypes(include = [np.number])

# Finding categorical features
categorical_data = df_train.select_dtypes(exclude = [np.number])

# Print how many numerical and categorical features do we have
print('There are {0} numerical and {1} categorical features in the training data'.\
      format(numeric_data.shape[1], categorical_data.shape[1]))

# Box plot to analyse the means of categorical features
# Null Hypothesis

# count plots of categorical features
f = pd.melt(df_train, id_vars = ['SalePrice'], value_vars = sorted(categorical_data))
g = sns.FacetGrid(f, col = 'variable', col_wrap = 3, sharex = False, sharey=False, size = 4)
g = g.map(sns.boxplot, 'value', 'SalePrice')
[plt.setp(ax.get_xticklabels(), rotation = 90) for ax in g.axes.flat]
g.fig.tight_layout()
plt.show()

# we notice the difference in the means... looking for features with variance

#ERROR!!!!!!!!!!!!


import scipy.stats
cat_features = categorical_data.columns

# FIlling nulls with 'missing' as value
df_train[cat_features] = df_train[cat_features].fillna('Missing')

# Onward...
anova = {'feature': [], 'f':[], 'p':[]}
for cat in cat_features:
  group_prices = []
  for group in df_train[cat].unique():
      group_prices.append(df_train[df_train[cat] == group]['SalePrice'].values)
  f, p = scipy.stats.f_oneway(*group_prices)
  anova['feature'].append(cat)
  anova['f'].append(f)
  anova['p'].append(p)
anova = pd.DataFrame(anova)
anova = anova[['feature', 'f', 'p']]
anova.sort_values('p', inplace = True)
 
p < 0.05 # if we get a p value which is dependent on the statistics so if p < 0.05 we will reject the null hypothesis

plt.figure(figsize = (8,5))
sns.barplot(anova.feature, np.log(1./anova['p']))
plt.xticks(rotation = 90)
plt.show()

# Handling missing values(locating and replacing misising values)
missing_values = (df_train[cat_features] == 'Missing').sum().sort_values(ascending = False)
missing_values[missing_values > 0]

df_train.loc[df_train.Electrical == 'Missing', 'Electrical'] = df_train.Electrical.mode()[0]

missing_data = df_train.isnull().sum() / df_train.shape[0]
missing_data[missing_data > 0].\
    sort_values(ascending = True).\
    plot(kind = 'barh', figsize = (10,6))
plt.title('Percentage of missing values')
plt.show()

# Lot Frontage (how can there be no street infront of the lot) Hence we replace it with the median value.
df_train.LotFrontage = df_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

# Garage Year built, if missing we can set it to zero
df_train.GarageYrBlt.fillna(0, inplace = True)

# Masonary Veneer Area here most values are zero
df_train.MasVnrArea.fillna(0, inplace = True)

# Let's closely look into the MasVnrType field here

# First let's correct our assignment
df_train.MasVnrType.replace({'Missing' : 'None'}, inplace = True)

# Second, we are going to replace them with the main value

df_train.loc[(df_train.MasVnrType == 'None') & (df_train.MasVnrArea > 1), 'MasVnrType'] = 'BrkFace' # most common
df_train.loc[(df_train.MasVnrType == 'None') & (df_train.MasVnrArea == 1), 'MasVnrArea'] = 0 # 1 sq ft is basically 0
for vnr_type in df_train.MasVnrType.unique():
    # so here we set the area equal to the mean of the given veneer type
    df_train.loc[(df_train.MasVnrType == vnr_type) & (df_train.MasVnrArea == 0), 'MasVnrArea'] = \
        df_train[df_train.MasVnrType == vnr_type].MasVnrArea.mean()

missing_data = df_train.isnull().sum() / df_train.shape[0]
missing_data

# Check wether duplicates are available

print('Train set duplicate IDs: {}'.format(df_train.duplicated('Id').sum()))
print('Test set duplicate IDs: {}'.format(df_train.duplicated('Id').sum()))

# checking and showing the outliers (high net worth people(expensive homes))

plt.figure(figsize=(10,5))
sns.regplot(df_train.GrLivArea, df_train.SalePrice, scatter_kws = {'alpha':0.3})
plt.show()

# Removing the houses which are more than 4000 sq ft.
df_train.drop(df_train[df_train.GrLivArea >= 4000].index, inplace = True)

# Calculating total square feet (area)
df_train['TotalSF'] = df_train.TotalBsmtSF + df_train.GrLivArea
df_train['TotalFloorSF'] = df_train['1stFlrSF'] + df_train['2ndFlrSF']
df_train['TotalPorchSF'] = df_train.OpenPorchSF + df_train.EnclosedPorch + \
    df_train['3SsnPorch'] + df_train['ScreenPorch']

# Create some boolean features (Yes-no type)

df_train['HasBasement'] = df_train.TotalBsmtSF.apply(lambda x: 1 if x > 0 else 0)
df_train['HasGarage'] = df_train.GarageArea.apply(lambda x: 1 if x > 0 else 0)
df_train['HasPorch'] = df_train.TotalPorchSF.apply(lambda x: 1 if x > 0 else 0)
df_train['HasPool'] = df_train.PoolArea.apply(lambda x: 1 if x > 0 else 0)
df_train['WasRemodeled'] = (df_train.YearRemodAdd != df_train.YearBuilt).astype(np.int64)
df_train['IsNew'] = (df_train.YearBuilt > 2000).astype(np.int64)
df_train['WasCompleted'] = (df_train.SaleCondition != 'Partial').astype(np.int64)

boolean_features = ['HasBasement', 'HasGarage', 'HasPorch', 'HasPool', 'WasRemodeled', 'IsNew', 'WasCompleted']

# Finding numerical features
numeric_data = df_train.select_dtypes(include = [np.number])

# Finding categorical features
categorical_data = df_train.select_dtypes(exclude = [np.number])

# Print how many numerical and categorical features do we have
print('There are {0} numerical and {1} categorical features in the training data'.\
      format(numeric_data.shape[1], categorical_data.shape[1]))

num_features = numeric_data.columns
cat_features = categorical_data.columns

num_features = [f for f in num_features if f not in boolean_features]

# Total Bathrooms

df_train['TotalBathrooms'] = df_train.FullBath + .5 * df_train.HalfBath + \
    df_train.BsmtFullBath + .5 * df_train.BsmtHalfBath

# Finding numerical features
numeric_data = df_train.select_dtypes(include = [np.number])

# Finding categorical features
categorical_data = df_train.select_dtypes(exclude = [np.number])

# Print how many numerical and categorical features do we have
print('There are {0} numerical and {1} categorical features in the training data'.\
      format(numeric_data.shape[1], categorical_data.shape[1]))

num_features = numeric_data.columns
cat_features = categorical_data.columns
num_features = [f for f in num_features if f not in boolean_features]

# Log Transformation
# one-hot encoding for handling our categorical features
# Data-modeling - lasso regression, ridge regression kind of analysis

features = num_features + ['SalePrice']
for f in features:
    df_train.loc[:,f] = np.log1p(df_train[f])

#y = df_train['SalePrice']
#df_train.drop('SalePrice', axis=1, inplace = True)
#df_train.drop('Id', axis=1, inplace = True)

#Get dummies one-hot encoding method
#model_data = pd.get_dummies(df_train).copy()

model_data.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = \
    train_test_split(model_data.copy(), y, test_size=0.3, random_state=42)
print('Shapes')
print('X_train:', X_train.shape)
print('X_val:', X_test.shape)
print('y_train:', y_train.shape)
print('y_val:', y_test.shape)

from sklearn.preprocessing import RobustScaler, StandardScaler

# Remove Id and SalePrice from num_features
num_features.remove('Id')
num_features.remove('SalePrice')

# Reason is to make feature means at 0 with standard deviation of 1
stdsc = StandardScaler()
X_train.loc[:,num_features] = stdsc.fit_transform(X_train[num_features])
X_train.loc[:,num_features] = stdsc.transform(X_train[num_features])

# Note we're only standardizing numerical features, not the dummy features.

from sklearn.model_selection import cross_val_score

def rsme(model, X,y):
    cv_scores = -cross_val_score(model, X,y, scoring = 'neg_mean_squared_error', cv=10)
    return np.sqrt(cv_scores)

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1,1.,5.,10.,25.], 'max_iter':[50000]}
lasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')
lasso.fit(X_train, y_train)
alpha = lasso.best_params_['alpha']

param_grid = {'alpha': [x/100. * alpha for x in range (50, 150, 5)], 'max_iter':[50000]}
lasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')
lasso.fit(X_train, y_train)
alpha = lasso.best_params_['alpha']
lasso = lasso.best_estimator_

print('Lasso -> Train RSME: {:.5f} | Test RSME: {:.5f} | alpha: {:.5f}'.format(
    rsme(lasso, X_train, y_train).mean(), rsme(lasso, X_test, y_test).mean(), alpha))

import seaborn as sns
coefs = pd.DataFrame({'coefs':lasso.coef_,'Positive':lasso.coef_ > 0}, index=X_train.columns)
coefs['coefs_abs'] = np.abs(coefs.coefs)
print('LASSO dropped {} of {} features.'.format(
    sum(coefs.coefs == 0), coefs.shape[0]))

top_coefs = coefs.sort_values('coefs_abs', ascending = False).head(20)
plt.figure(figsize= (8,10))
sns.barplot(top_coefs.coefs_abs, top_coefs.index, orient = 'h', hue = top_coefs.Positive)
plt.show()

# Now let's apply machine learning XGBoost library

import xgboost as xgb
regr = xgb.XGBRegressor(colsample_bytree = 0.2,
                        gamma = 0.0,
                        learning_rate = 0.05,
                        max_depth = 6,
                        min_child_weight = 1.5,
                        n_estimators = 7200,
                        reg_alpha = 0.9,
                        reg_lambda = 0.6,
                        subsample = 0.2,
                        seed = 42,
                        silent = 1)
regr.fit(X_train[top_coefs.index], y_train)

from sklearn.metrics import mean_squared_error
def rmse(y_test, y_pred):
    return np.sqrt(mean_squared_error(y_test, y_pred))
  
# run prediction on training set to get an idea of how well it does
y_pred = regr.predict(X_test[top_coefs.index])

print('XGBoost score on training set:', rmse(y_test, y_pred))
